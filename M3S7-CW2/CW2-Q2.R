#Q2
#(a)
"rrayleigh" <- function (n,theta) {
    u <- runif(n,0,1)
    sqrt(-2*log(u))/sqrt(2*theta)
  }

"rsample"<-function(n,theta1,theta2){
  #class1 sample
  rray.x1<-rrayleigh(n,theta1)
  rray.y1<-rrayleigh(n,theta1)
  
  rray.x2<-rrayleigh(n,theta2)
  rray.y2<-rrayleigh(n,theta2)
  
  rrayc1.class<-rep(1,n)
  rrayc2.class<-rep(2,n)
  
  rand.sample<-data.frame(rray.x1,rray.y1,rrayc1.class,rray.x2,rray.y2,rrayc2.class)
}
#the function returns a data frame for a better clarity and readability of my code. 
#I later transform the samples generated by this function into matrices or lists to answer the 
#following questions in an easier way.


#(b)-(c)
#the following function returns a value for the function |e_B-0.15| for a given theta1.
#we then use the golden ratio bracketing algorithm to find the value for theta1 
#that minimizes it

"bayes.error"<-function(theta1){
  theta2=1
  Tmin<-sqrt(log(theta1/theta2)*(2/(theta1-theta2)))
 
  cdf.c1<-theta1*(Tmin^2)*exp(-theta1*Tmin^2)+exp(-theta1*Tmin^2)
  cdf.c2<-1-theta2*(Tmin^2)*exp(-theta2*Tmin^2)-exp(-theta2*Tmin^2)
  b.error<-abs(0.5*(cdf.c1+cdf.c2)-0.15)
  return(b.error)
}


#golden search algorithm
grbrack<-function(f,x1,x3,eps){
  ratio<-(1+sqrt(5))/2
  x2<-x1+(x3-x1)/ratio
  x2
  i=0
  
  while ((x3-x1)>eps){
    i=i+1   
  
    f.x1=f(x1)
    f.x2=f(x2)
    f.x3=f(x3)
    if (f.x2<f.x1 && f.x2<f.x3){
      x3<-x2
      x2<-x1+(x3-x1)/ratio
    }
    else{
      x1<-x2
      x2<-x1+(x3-x1)/ratio 
    }  
  }
  return(x2)
}
grbrack(bayes.error,3,10,0.0000001)

#(d)
#the following function computes the discriminant score conditional on the 2-classes 
#it then compares the 2 discriminant scores and allocates the tested point to
#the class with the highest score.
#discriminant function used: g_i(x)=log(p(c_i))+log(p(x|c_i))
"discriminant"<-function(x,y,theta1,theta2){
  
  p.c1<-4*theta1^2*x*y*exp(-theta1*(x^2+y^2))
  p.c2<-4*theta2^2*x*y*exp(-theta2*(x^2+y^2))
  
  g1<-log(0.5)+log(p.c1)
  g2<-log(0.5)+log(p.c2)
  discr<-g1-g2
  if (g1>=g2){
    class<-1
  }
  else{
    class<-2
  }
  return(class)
}

#points<-rsample(100,4,2)
# test<-discriminant(points$rray.x2[5],points$rray.y2[5],4,2)
# test

#(e)
#f(x,y) = p(C1)f(x,y|C1)+p(C2)f(x,y|C2)

theta1=4
theta2=2
xvals=seq(0,1.4,0.02)
yvals=xvals
uncond.cdf<-matrix(0,nrow=length(xvals),ncol=length(yvals))
length(xvals)
for (i in 1:length(xvals)){
  for (j in 1:length(yvals)){
    f.c1<-4*theta1^2*xvals[i]*yvals[j]*exp(-theta1*(xvals[i]^2+yvals[j]^2))
    f.c2<-4*theta2^2*xvals[i]*yvals[j]*exp(-theta2*(xvals[i]^2+yvals[j]^2))
    print(f.c1)
    uncond.cdf[i,j]<-0.5*(f.c1+f.c2)
  }
}
uncond.cdf
obs50<-rsample(50,4,2)

contour(xvals,yvals,uncond.cdf,main='Contour plot with data and decision boundary',xlab='x',ylab='y')
points(obs50$rray.x1,obs50$rray.y1,col="red",pch=4)
points(obs50$rray.x2,obs50$rray.y2,col="blue",pch=4)
Tmin<-sqrt(log(4/2)*(2/(4-2)))
require("plotrix")
draw.circle(0,0,Tmin,nv=100,border="dark green",lty=1,lwd=2)
legend("topright",legend=c("C1","C2","Tmin"),col=c("red","blue","dark green"),lty=1,bty='n')


#(g)
"ml"<-function(data){
  n<-length(data$rray.x1)
  thetahat.c1<-2*n/sum((data$rray.x1)^2+(data$rray.y1)^2)
  thetahat.c2<-2*n/sum((data$rray.x2)^2+(data$rray.y2)^2)
  thetahat.est<-cbind(thetahat.c1,thetahat.c2)
  return(thetahat.est)
}


ml.est<-ml(obs50)
ml.est

"eval.discr"<-function(data,estimate){
  discr.alloc1<-rep(0,length(data$rray.x1))
  discr.alloc2<-rep(0,length(data$rray.x1))
  for (i in 1:length(data$rray.x1)){
    discr.alloc1[i]<-discriminant(data$rray.x1[i],data$rray.y1[i],estimate[1],estimate[2])
    discr.alloc2[i]<-discriminant(data$rray.x2[i],data$rray.y2[i],4,2)
  }
  data.new<-data.frame(data$rray.x1,data$rray.y1,data$rrayc1.class,discr.alloc1,data$rray.x2,data$rray.y2,data$rrayc2.class,discr.alloc2)
  return(data.new)
}
points.new<-eval.discr(obs50,ml.est)
#comparing the class allocated by the ml classifier and the true class:

tablec1<-table(points.new$data.rrayc1.class,points.new$discr.alloc1)
tablec2<-table(points.new$data.rrayc2.class,points.new$discr.alloc2)
errorc1<-tablec1[2]/(tablec1[1]+tablec1[2])
errorc2<-tablec2[1]/(tablec2[1]+tablec2[2])
errorc1
errorc2
classifier.error<-0.5*(errorc2+errorc1)
#the error of this classifier is the following:
classifier.error


#Q2h
training.sample<-rsample(100,4,2)
test.sample<-rsample(5000,4,2)

#Estimative using ML
training.ml<-ml(training.sample)
test.ml<-ml(test.sample)

training.eval<-eval.discr(training.sample,training.ml)
test.eval<-eval.discr(test.sample,test.ml)

training.class.tot<-c(training.eval$data.rrayc1.class,training.eval$data.rrayc2.class)
training.class.est<-c(training.eval$discr.alloc1,training.eval$discr.alloc2)
test.class.tot<-c(test.eval$data.rrayc1.class,test.eval$data.rrayc2.class)
test.class.est<-c(test.eval$discr.alloc1,test.eval$discr.alloc2)

training.ml.table<-table(training.class.tot,training.class.est)
test.ml.table<-table(test.class.tot,test.class.est)

training.ml.error<-(training.ml.table[2,1]+training.ml.table[1,2])/200
test.ml.error<-(test.ml.table[2,1]+test.ml.table[1,2])/10000

training.ml.error
test.ml.error

#LDA
#training
training2.data<-cbind(c(training.sample$rray.x1,training.sample$rray.x2),c(training.sample$rray.y1,training.sample$rray.y2))
training2.data

training2.class<-c(training.sample$rrayc1.class,training.sample$rrayc2.class)
#training2.class
training2.lda<-lda(training2.data,training2.class)
#training2.lda

prediction.training2<-predict(training2.lda)
training2.lda.table<-table(prediction.training2$class,training2.class)
training2.lda.error<-(training2.lda.table[1,2]+training2.lda.table[2,1])/200
training2.lda.error

#LDA-testing
test2.data<-cbind(c(test.sample$rray.x1,test.sample$rray.x2),c(test.sample$rray.y1,test.sample$rray.y2))
test2.class<-c(test.sample$rrayc1.class,test.sample$rrayc2.class)

test2.lda<-lda(test2.data,test2.class)
prediction.test2<-predict(test2.lda)
test2.lda.table<-table(prediction.test2$class,test2.class)
test2.lda.error<-(test2.lda.table[1,2]+test2.lda.table[2,1])/10000
test2.lda.error

#QDA
#training
training2.qda<-qda(training2.data,training2.class)
training2.predict.qda<-predict(training2.qda)
training2.table.qda<-table(training2.predict.qda$class,training2.class)
training2.qda.error<-(training2.table.qda[1,2]+training2.table.qda[2,1])/200
training2.qda.error

#testing
test2.qda<-qda(test2.data,test2.class)
test2.predict.qda<-predict(test2.qda)
test2.table.qda<-table(test2.predict.qda$class,test2.class)
test2.qda.error<-(test2.table.qda[1,2]+test2.table.qda[2,1])/10000
test2.qda.error

